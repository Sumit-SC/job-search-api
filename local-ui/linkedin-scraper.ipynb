{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# LinkedIn Jobs Scraper\n",
    "\n",
    "A Python scraper for LinkedIn job postings using BeautifulSoup.\n",
    "\n",
    "## Features\n",
    "- Scrapes job listings from LinkedIn\n",
    "- Configurable search parameters (keywords, location, max jobs)\n",
    "- Rate limiting and retry logic\n",
    "- Exports results to JSON\n",
    "- Clean, structured data output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install requests==2.32.3 beautifulsoup4==4.12.3 tenacity==9.0.0 -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "from urllib.parse import quote\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util import Retry\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config",
   "metadata": {},
   "source": [
    "## Configuration & Data Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class JobData:\n",
    "    title: str\n",
    "    company: str\n",
    "    location: str\n",
    "    job_link: str\n",
    "    posted_date: str\n",
    "\n",
    "\n",
    "class ScraperConfig:\n",
    "    BASE_URL = \"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search\"\n",
    "    JOBS_PER_PAGE = 25\n",
    "    MIN_DELAY = 2\n",
    "    MAX_DELAY = 5\n",
    "    RATE_LIMIT_DELAY = 30\n",
    "    RATE_LIMIT_THRESHOLD = 10\n",
    "\n",
    "    HEADERS = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"DNT\": \"1\",\n",
    "        \"Cache-Control\": \"no-cache\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scraper-class",
   "metadata": {},
   "source": [
    "## LinkedIn Scraper Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scraper-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkedInJobsScraper:\n",
    "    def __init__(self):\n",
    "        self.session = self._setup_session()\n",
    "\n",
    "    def _setup_session(self) -> requests.Session:\n",
    "        session = requests.Session()\n",
    "        retries = Retry(\n",
    "            total=5, backoff_factor=0.5, status_forcelist=[429, 500, 502, 503, 504]\n",
    "        )\n",
    "        session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "        return session\n",
    "\n",
    "    def _build_search_url(self, keywords: str, location: str, start: int = 0) -> str:\n",
    "        params = {\n",
    "            \"keywords\": keywords,\n",
    "            \"location\": location,\n",
    "            \"start\": start,\n",
    "        }\n",
    "        return f\"{ScraperConfig.BASE_URL}?{'&'.join(f'{k}={quote(str(v))}' for k, v in params.items())}\"\n",
    "\n",
    "    def _clean_job_url(self, url: str) -> str:\n",
    "        return url.split(\"?\")[0] if \"?\" in url else url\n",
    "\n",
    "    def _extract_job_data(self, job_card: BeautifulSoup) -> Optional[JobData]:\n",
    "        try:\n",
    "            title = job_card.find(\"h3\", class_=\"base-search-card__title\").text.strip()\n",
    "            company = job_card.find(\n",
    "                \"h4\", class_=\"base-search-card__subtitle\"\n",
    "            ).text.strip()\n",
    "            location = job_card.find(\n",
    "                \"span\", class_=\"job-search-card__location\"\n",
    "            ).text.strip()\n",
    "            job_link = self._clean_job_url(\n",
    "                job_card.find(\"a\", class_=\"base-card__full-link\")[\"href\"]\n",
    "            )\n",
    "            posted_date = job_card.find(\"time\", class_=\"job-search-card__listdate\")\n",
    "            posted_date = posted_date.text.strip() if posted_date else \"N/A\"\n",
    "\n",
    "            return JobData(\n",
    "                title=title,\n",
    "                company=company,\n",
    "                location=location,\n",
    "                job_link=job_link,\n",
    "                posted_date=posted_date,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract job data: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _fetch_job_page(self, url: str) -> BeautifulSoup:\n",
    "        try:\n",
    "            response = self.session.get(url, headers=ScraperConfig.HEADERS)\n",
    "            if response.status_code != 200:\n",
    "                raise RuntimeError(\n",
    "                    f\"Failed to fetch data: Status code {response.status_code}\"\n",
    "                )\n",
    "            return BeautifulSoup(response.text, \"html.parser\")\n",
    "        except requests.RequestException as e:\n",
    "            raise RuntimeError(f\"Request failed: {str(e)}\")\n",
    "\n",
    "    def scrape_jobs(\n",
    "        self, keywords: str, location: str, max_jobs: int = 100\n",
    "    ) -> List[JobData]:\n",
    "        all_jobs = []\n",
    "        start = 0\n",
    "\n",
    "        while len(all_jobs) < max_jobs:\n",
    "            try:\n",
    "                url = self._build_search_url(keywords, location, start)\n",
    "                soup = self._fetch_job_page(url)\n",
    "                job_cards = soup.find_all(\"div\", class_=\"base-card\")\n",
    "\n",
    "                if not job_cards:\n",
    "                    break\n",
    "                for card in job_cards:\n",
    "                    job_data = self._extract_job_data(card)\n",
    "                    if job_data:\n",
    "                        all_jobs.append(job_data)\n",
    "                        if len(all_jobs) >= max_jobs:\n",
    "                            break\n",
    "                print(f\"Scraped {len(all_jobs)} jobs...\")\n",
    "                start += ScraperConfig.JOBS_PER_PAGE\n",
    "                time.sleep(\n",
    "                    random.uniform(ScraperConfig.MIN_DELAY, ScraperConfig.MAX_DELAY)\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Scraping error: {str(e)}\")\n",
    "                break\n",
    "        return all_jobs[:max_jobs]\n",
    "\n",
    "    def save_results(\n",
    "        self, jobs: List[JobData], filename: str = \"linkedin_jobs.json\"\n",
    "    ) -> None:\n",
    "        if not jobs:\n",
    "            return\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump([vars(job) for job in jobs], f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Saved {len(jobs)} jobs to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-config",
   "metadata": {},
   "source": [
    "## Configure Search Parameters\n",
    "\n",
    "Edit these values to customize your search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-params",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search parameters - Edit these!\n",
    "SEARCH_KEYWORDS = \"AI/ML Engineer\"  # Job title or keywords\n",
    "SEARCH_LOCATION = \"London\"  # Location (city, state, country, or \"Remote\")\n",
    "MAX_JOBS = 50  # Maximum number of jobs to scrape\n",
    "\n",
    "print(f\"Search Configuration:\")\n",
    "print(f\"  Keywords: {SEARCH_KEYWORDS}\")\n",
    "print(f\"  Location: {SEARCH_LOCATION}\")\n",
    "print(f\"  Max Jobs: {MAX_JOBS}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-scraper",
   "metadata": {},
   "source": [
    "## Run Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-scraper-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scraper\n",
    "scraper = LinkedInJobsScraper()\n",
    "\n",
    "# Run the scraper\n",
    "print(f\"Starting LinkedIn job scrape...\")\n",
    "print(f\"Searching for: '{SEARCH_KEYWORDS}' in '{SEARCH_LOCATION}'\")\n",
    "print(f\"Target: {MAX_JOBS} jobs\\n\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "jobs = scraper.scrape_jobs(\n",
    "    keywords=SEARCH_KEYWORDS,\n",
    "    location=SEARCH_LOCATION,\n",
    "    max_jobs=MAX_JOBS\n",
    ")\n",
    "end_time = datetime.now()\n",
    "\n",
    "duration = (end_time - start_time).total_seconds()\n",
    "print(f\"\\n‚úÖ Scraping complete!\")\n",
    "print(f\"   Found: {len(jobs)} jobs\")\n",
    "print(f\"   Duration: {duration:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "display-results",
   "metadata": {},
   "source": [
    "## Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "display-results-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few results\n",
    "if jobs:\n",
    "    print(f\"\\nüìä Sample Results (showing first 5):\\n\")\n",
    "    for i, job in enumerate(jobs[:5], 1):\n",
    "        print(f\"{i}. {job.title}\")\n",
    "        print(f\"   Company: {job.company}\")\n",
    "        print(f\"   Location: {job.location}\")\n",
    "        print(f\"   Posted: {job.posted_date}\")\n",
    "        print(f\"   Link: {job.job_link}\")\n",
    "        print()\n",
    "    \n",
    "    if len(jobs) > 5:\n",
    "        print(f\"... and {len(jobs) - 5} more jobs\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No jobs found. Try different keywords or location.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataframe",
   "metadata": {},
   "source": [
    "## View as DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataframe-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrame for easier viewing\n",
    "if jobs:\n",
    "    df = pd.DataFrame([vars(job) for job in jobs])\n",
    "    print(f\"\\nüìã All {len(df)} Jobs:\\n\")\n",
    "    display(df)\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nüìà Summary:\")\n",
    "    print(f\"   Total jobs: {len(df)}\")\n",
    "    print(f\"   Unique companies: {df['company'].nunique()}\")\n",
    "    print(f\"   Unique locations: {df['location'].nunique()}\")\n",
    "    \n",
    "    # Top companies\n",
    "    if len(df) > 0:\n",
    "        print(f\"\\nüè¢ Top Companies:\")\n",
    "        top_companies = df['company'].value_counts().head(5)\n",
    "        for company, count in top_companies.items():\n",
    "            print(f\"   {company}: {count} jobs\")\n",
    "else:\n",
    "    print(\"No data to display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-results",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to JSON\n",
    "if jobs:\n",
    "    json_filename = f\"linkedin_jobs_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    scraper.save_results(jobs, json_filename)\n",
    "    \n",
    "    # Also save to CSV\n",
    "    csv_filename = f\"linkedin_jobs_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    df = pd.DataFrame([vars(job) for job in jobs])\n",
    "    df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "    print(f\"\\nüíæ Also saved to CSV: {csv_filename}\")\n",
    "else:\n",
    "    print(\"No jobs to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-quick",
   "metadata": {},
   "source": [
    "## Quick Test (Small Sample)\n",
    "\n",
    "Run this cell for a quick test with just 5 jobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test with minimal jobs\n",
    "test_scraper = LinkedInJobsScraper()\n",
    "test_jobs = test_scraper.scrape_jobs(\n",
    "    keywords=\"Data Analyst\",\n",
    "    location=\"San Francisco\",\n",
    "    max_jobs=5\n",
    ")\n",
    "\n",
    "if test_jobs:\n",
    "    print(f\"\\n‚úÖ Quick test successful! Found {len(test_jobs)} jobs:\\n\")\n",
    "    for job in test_jobs:\n",
    "        print(f\"  ‚Ä¢ {job.title} at {job.company} ({job.location})\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Quick test found no jobs. This might indicate:\")\n",
    "    print(\"   - LinkedIn API changes\")\n",
    "    print(\"   - Rate limiting\")\n",
    "    print(\"   - Network issues\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
